# ğŸš€ Automated Big Data ETL Pipeline: Scalable Processing with AWS EMR, Airflow, and Snowflake â„ï¸

## ğŸ¢ Introduction

This project demonstrates how to build an **automated big data ETL pipeline** using **Apache Airflow ğŸ“š**, **Amazon EMR ğŸ“Š**, **Amazon S3 ğŸ’¾**, and **Snowflake â„ï¸**. The pipeline:

- ğŸ­ **Creates an EMR Cluster**
- ğŸ”¥ Runs **Spark jobs** to process raw data
- ğŸ’¾ Stores the transformed data in **Amazon S3**
- â„ï¸ Loads the processed data into **Snowflake**
- ğŸš« Shuts down the **EMR cluster** after processing

The DAG is orchestrated using **Apache Airflow**, automating the entire ETL process. âš™ï¸

---

## ğŸ“ Repository Structure

### ğŸ› ï¸ Clone the Project

```sh
git clone https://github.com/Abd-al-RahmanH/Automated-Big-Data-ETL-Pipeline-Scalable-Processing-with-AWS-EMR-Airflow-and-Snowflake.git

cd Automated-Big-Data-ETL-Pipeline-Scalable-Processing-with-AWS-EMR-Airflow-and-Snowflake
```

### ğŸ“Œ Project Files

```
FINAL_EMR_REDFIN/
â”‚-- dags/
â”‚   â”œ-- redfin_analytics.py       # Airflow DAG for EMR cluster and Spark jobs
â”‚
â”‚-- scripts/
â”‚   â”œ-- ingest.sh                 # Shell script for data ingestion
â”‚   â”œ-- transform_redfin_data.py  # Spark script for data transformation
â”‚
â”‚-- commands.txt  # Commands for setting up AWS & Airflow
â”‚-- README.md     # Project Documentation
â”‚-- requirements.txt  # Python dependencies
```

---

## âš¡ Prerequisites

Before running the project, ensure you have the following:

âœ”ï¸ **AWS Account** with permissions for VPC, S3, EMR, IAM, and Snowflake integration  
âœ”ï¸ **S3 Bucket** for raw and processed data  
âœ”ï¸ **Apache Airflow ğŸ“š** installed and configured  
âœ”ï¸ **AWS credentials** configured in Airflow  
âœ”ï¸ **Snowflake â„ï¸ Account** with the necessary database and tables  

---

## ğŸ›¡ï¸ Step 1: Setting Up a VPC

Before creating an S3 bucket, set up a **VPC (Virtual Private Cloud)** to ensure a secure and controlled networking environment.

### ğŸ”¹ Create a VPC

```sh
aws ec2 create-vpc --cidr-block 10.0.0.0/16
```

### ğŸ”¹ Create a Subnet

```sh
aws ec2 create-subnet --vpc-id <your-vpc-id> --cidr-block 10.0.1.0/24
```

### ğŸ”¹ Create an Internet Gateway

```sh
aws ec2 create-internet-gateway
aws ec2 attach-internet-gateway --vpc-id <your-vpc-id> --internet-gateway-id <your-igw-id>
```

### Create in UI by follow the image

![](images/Screenshot_68.jpg)
![](images/Screenshot_69.jpg)

---

## ğŸ’¾ Step 2: Setting Up an S3 Bucket

Create an **S3 bucket** with the following structure:

```
s3://redfin-emr-project/
â”œ-- raw-data/            # Stores raw CSV data
â”œ-- scripts/             # Stores Spark and ingestion scripts
â”œ-- transformed-data/    # Stores processed parquet files
â”œ-- emr-logs/            # Stores EMR logs
```

![](images/Screenshot_86.jpg)

Upload necessary scripts to the S3 bucket:

```sh
aws s3 cp scripts/ s3://redfin-emr-project/scripts/ --recursive
```

---

## ğŸ”„ Step 3: Configuring Airflow DAG

The **redfin_analytics.py** DAG automates the ETL workflow.

### ğŸ”— DAG Flow:

1ï¸âƒ£ **Start pipeline** (`tsk_start_pipeline`)  
2ï¸âƒ£ **Create EMR cluster** (`tsk_create_emr_cluster`)  
3ï¸âƒ£ **Check cluster status** (`tsk_is_emr_cluster_created`)  
4ï¸âƒ£ **Submit Spark jobs** (`tsk_add_extraction_step`, `tsk_add_transformation_step`)  
5ï¸âƒ£ **Check Spark job completion** (`tsk_is_extraction_completed`, `tsk_is_transformation_completed`)  
6ï¸âƒ£ **Load data into Snowflake** (`tsk_load_to_snowflake`)  
7ï¸âƒ£ **Terminate EMR cluster** (`tsk_remove_cluster`)  
8ï¸âƒ£ **End pipeline** (`tsk_end_pipeline`)  

### âš¡ Key Configurations in the DAG

Ensure to update the following in `redfin_analytics.py`:

- âœ… **Change the Subnet ID**: `Ec2SubnetId": "subnet-XXXXXX"`
- âœ… **Update EMR Cluster Name**: `"Name": "Your-EMR-Cluster-Name"`
- âœ… **Specify Your Key Name**: `"Ec2KeyName": "YourKeyName"`
- âœ… **Update S3 Bucket Name**: `s3://your-default-bucket-name/`

## Make sure u add the role from the commnads.txt

![](images/Screenshot_78.jpg)

---

## â–¶ï¸ Step 4: Running the DAG

1ï¸âƒ£ Start the **Airflow webserver and scheduler**:

```sh
airflow webserver -p 8080 &
airflow scheduler &
```

2ï¸âƒ£ Open **Airflow UI** (http://localhost:8080)  and get th password from the terminal
3ï¸âƒ£ Enable the DAG: `redfin_analytics_spark_job_dag` by clicking play button

![](images/Screenshot_75.jpg)

4ï¸âƒ£ Trigger the DAG manually or wait for its scheduled execution  

## â–¶ï¸ Step 5: The job runs
1. After you executed the job will creat the **EMR cluster**

![](images/Screenshot_77.jpg)

2. It will fetch the raw data

![](images/Screenshot_83.jpg)

3. It will passed to spark jobs from processing and put the changed job in transformed data s3 bucket

![](images/Screenshot_84.jpg)

4. After that the emr cluster will be terminated automatically

![](images/Screenshot_79.jpg)

![](images/Screenshot_80.jpg)


5. And the job is completed

![](images/Screenshot_81.jpg)

---

## ğŸ› ï¸ Step 5: Troubleshooting

### â— Common Issues & Fixes

- âŒ **EMR creation fails?** Ensure correct **subnet ID** and **AWS key pair**.
- âŒ **S3 path issues?** Verify bucket structure and permissions.
- ğŸ” **Airflow task failure?** Check **task logs** and IAM roles.

---

## âœ… Conclusion

âœ”ï¸ Fully automated **ETL pipeline** with Apache Airflow ğŸ“š  
âœ”ï¸ Scalable **Spark jobs** on AWS EMR ğŸ“Š  
âœ”ï¸ Secure **data storage** in Amazon S3 & Snowflake â„ï¸  
âœ”ï¸ Seamless **orchestration** for big data processing ğŸ¯  

For any queries, feel free to contribute to the project! ğŸ™Œ

