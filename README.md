# ğŸš€ Automated Big Data ETL Pipeline: Scalable Processing with AWS EMR, Airflow, and Snowflake â„ï¸

## ğŸ—ï¸ Introduction

This project demonstrates how to build an **automated big data ETL pipeline** using **Apache Airflow ğŸŒ€**, **Amazon EMR ğŸ“Š**, **Amazon S3 ğŸ—„ï¸**, and **Snowflake â„ï¸**. The pipeline:

- ğŸ­ **Creates an EMR Cluster**
- ğŸ”¥ Runs **Spark jobs** to process raw data
- ğŸ—„ï¸ Stores the transformed data in **Amazon S3**
- â„ï¸ Loads the processed data into **Snowflake**
- ğŸ›‘ Shuts down the **EMR cluster** after processing

The DAG is orchestrated using **Apache Airflow**, automating the entire ETL process. âš™ï¸

---

## ğŸ“ Repository Structure

### ğŸ› ï¸ Clone the Project

```sh
git clone https://github.com/Abd-al-RahmanH/Automated-Big-Data-ETL-Pipeline-Scalable-Processing-with-AWS-EMR-Airflow-and-Snowflake.git
cd Automated-Big-Data-ETL-Pipeline-Scalable-Processing-with-AWS-EMR-Airflow-and-Snowflake
```

### ğŸ“Œ Project Files

```
FINAL_EMR_REDFIN/
â”‚â”€â”€ dags/
â”‚   â”œâ”€â”€ redfin_analytics.py       # Airflow DAG for EMR cluster and Spark jobs
â”‚
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest.sh                 # Shell script for data ingestion
â”‚   â”œâ”€â”€ transform_redfin_data.py  # Spark script for data transformation
â”‚
â”‚â”€â”€ commands.txt  # Commands for setting up AWS & Airflow
â”‚â”€â”€ README.md     # Project Documentation
â”‚â”€â”€ requirements.txt  # Python dependencies
```

---

## âš¡ Prerequisites

Before running the project, ensure you have the following:

âœ”ï¸ **AWS Account** with permissions for S3, EMR, IAM, and Snowflake integration  
âœ”ï¸ **S3 Bucket** for raw and processed data  
âœ”ï¸ **Apache Airflow ğŸŒ€** installed and configured  
âœ”ï¸ **AWS credentials** configured in Airflow  
âœ”ï¸ **Snowflake â„ï¸ Account** with the necessary database and tables  

---

## ğŸ“¦ Step 1: Setting Up S3 Bucket

Create an **S3 bucket** with the following structure:

```
s3://redfin-emr-project/
â”œâ”€â”€ raw-data/            # Stores raw CSV data
â”œâ”€â”€ scripts/             # Stores Spark and ingestion scripts
â”œâ”€â”€ transformed-data/    # Stores processed parquet files
â”œâ”€â”€ emr-logs/            # Stores EMR logs
```

Upload necessary scripts to the S3 bucket:

```sh
aws s3 cp scripts/ s3://redfin-emr-project/scripts/ --recursive
```

---

## ğŸ”„ Step 2: Configuring Airflow DAG

The **redfin_analytics.py** DAG automates the ETL workflow.

### ğŸ”— DAG Flow:

1ï¸âƒ£ **Start pipeline** (`tsk_start_pipeline`)  
2ï¸âƒ£ **Create EMR cluster** (`tsk_create_emr_cluster`)  
3ï¸âƒ£ **Check cluster status** (`tsk_is_emr_cluster_created`)  
4ï¸âƒ£ **Submit Spark jobs** (`tsk_add_extraction_step`, `tsk_add_transformation_step`)  
5ï¸âƒ£ **Check Spark job completion** (`tsk_is_extraction_completed`, `tsk_is_transformation_completed`)  
6ï¸âƒ£ **Load data into Snowflake** (`tsk_load_to_snowflake`)  
7ï¸âƒ£ **Terminate EMR cluster** (`tsk_remove_cluster`)  
8ï¸âƒ£ **End pipeline** (`tsk_end_pipeline`)  

---

## â–¶ï¸ Step 3: Running the DAG

1ï¸âƒ£ Start the **Airflow webserver and scheduler**:

```sh
airflow webserver -p 8080 &
airflow scheduler &
```

2ï¸âƒ£ Open **Airflow UI** (http://localhost:8080)  
3ï¸âƒ£ Enable the DAG: `redfin_analytics_spark_job_dag`  
4ï¸âƒ£ Trigger the DAG manually or wait for its scheduled execution  

---

## ğŸ› ï¸ Step 4: Troubleshooting

### â— Common Issues & Fixes

- âŒ **EMR creation fails?** Ensure correct **subnet ID** and **AWS key pair**.
- ğŸš« **S3 path issues?** Verify bucket structure and permissions.
- ğŸ” **Airflow task failure?** Check **task logs** and IAM roles.

---

## âœ… Conclusion

âœ”ï¸ Fully automated **ETL pipeline** with Apache Airflow ğŸŒ€  
âœ”ï¸ Scalable **Spark jobs** on AWS EMR ğŸ“Š  
âœ”ï¸ Secure **data storage** in Amazon S3 & Snowflake â„ï¸  
âœ”ï¸ Seamless **orchestration** for big data processing ğŸ¯  

For any queries, feel free to contribute to the project! ğŸ™Œ

